# API Keys and URLs
OPENAI_API_KEY=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 # UXResearch0903@gmail.com
MONGO_URL=mongodb+srv://uxresearch0903:fuBkhmu455d5G2aD@ragcluster.2v3o8.mongodb.net/?retryWrites=true&w=majority&appName=RAGCluster  # MongoDB connection URL
DB_NAME=rag_db  # Database name for MongoDB
SARVAM_API_URL="https://api.sarvam.ai/speech-to-text"  # Sarvam API URL for speech-to-text
SARVAM_API_KEY="8eba4738-5dc6-4ae6-9f87-323802de9966"  # Sarvam API Key
S3_TIMEOUT=60  # Timeout for S3 requests in seconds
AWS_REGION='ap-south-1'  # AWS region for services
AWS_ACCESS_KEY_ID=AKIA5JMST4UYY32PSVAA  # AWS Access Key ID
AWS_SECRET_ACCESS_KEY=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  # AWS Secret Access Key

# Question Generation Configuration
QUESTION_PROMPT=Based on the following transcript, generate as many detailed questions as possible that been addressed in the conversation. Ignore questions which are about the conversation fluff and only acculumate ones pertaining to the important aspects of the topic at hand. # Prompt for generating questions
QUESTION_PROMPT_FORMAT=Give the questions in a dictionary format such that the output can be put into the python eval function to make a dictionary without manual editing. Output it as a text and not in code. make question number as key for each question  # Format for question output
QUESTION_PROMPT_ROLE=system  # Role for question prompt
QUESTION_MODEL="gpt-4o"  # Model used for question generation
QUESTION_MAX_TOKENS=1000  # Maximum tokens for question generation

# Question Aggregation Configuration
QUESTION_AGG_PROMPT=Based on the following set of questions corresponding to all the transcripts, generate <n> questions asked most commonly across these trasncripts. You can combine multiple similar questions and reword it. Goal is to have <n> questions that fairly represent the topics addressed in each transcript. # Prompt for generating questions
QUESTION_AGG_PROMPT_FORMAT=Give the questions in a dictionary format such that the output can be put into the python eval function to make a dictionary without manual editing. Output it as a text and not in code. make question number as key for each question  # Format for question output
QUESTION_AGG_PROMPT_ROLE=system  # Role for question prompt
QUESTION_AGG_MODEL="gpt-4o"  # Model used for question generation
QUESTION_AGG_MAX_TOKENS=300  # Maximum tokens for question generation

# Answer Extraction Configuration
QA_PROMPT=Based on the given context, answer the question  # Prompt for extracting answers
QA_PROMPT_FORMAT=Give the answer in a dictionary format such that the output can be put into the python eval function to make a dictionary without manual editing. Output it as a text and not in code. dictionary should be having one key called answer and the answer should be its value  # Format for answer output
QA_PROMPT_ROLE=system  # Role for QA prompt
QA_MODEL="gpt-4o"  # Model used for answer extraction
QA_MAX_TOKENS=100  # Maximum tokens for answer extraction

# Grid Analysis Configuration
QA_GRID_PROMPT=I am parsing a dictionary of questions and a transcript as context. Answer each question for the given transcript context  # Prompt for grid analysis
QA_GRID_PROMPT_FORMAT=Give the answer in a dictionary format such that the output can be put into the python eval function to make a dictionary without manual editing. Output it as a text and not in code. dictionary should be having key as the question number and the value should be its answer  # Format for grid analysis output
QA_GRID_PROMPT_ROLE=system  # Role for grid analysis prompt
QA_GRID_MODEL="gpt-4o"  # Model used for grid analysis
QA_GRID_MAX_TOKENS=1000  # Maximum tokens for grid analysis

# Chat Configuration
CHAT_PROMPT=Based on the given context, answer the question  # Prompt for chat responses
CHAT_PROMPT_FORMAT="Let the output be of maximum 1000 tokens"  # Format for chat response output
CHAT_PROMPT_ROLE=system  # Role for chat prompt
CHAT_MODEL="gpt-4o"  # Model used for chat responses
CHAT_MAX_TOKENS=1000  # Maximum tokens for chat responses
CHAT_CONTEXT_REPEAT=1

# Cache and History Configuration
CACHE_TIMER=60  # Cache cleanup timer in seconds
MAX_CHAT_HISTORY_SAVE_LENGTH=10  # Maximum length of chat history to save
